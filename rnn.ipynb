{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "def create_train_tfdata(train_feat_dict, train_target_tensor,\n",
    "                        batch_size, buffer_size=None):\n",
    "    \"\"\"\n",
    "    Create train tf dataset for model train input\n",
    "    :param train_feat_dict: dict, containing the features tensors for train data\n",
    "    :param train_target_tensor: np.array(), the training TARGET tensor\n",
    "    :param batch_size: (int) size of the batch to work with\n",
    "    :param buffer_size: (int) Optional. Default is None. Size of the buffer\n",
    "    :return: (tuple) 1st element is the training dataset,\n",
    "                     2nd is the number of steps per epoch (based on batch size)\n",
    "    \"\"\"\n",
    "    if buffer_size is None:\n",
    "        buffer_size = batch_size*50\n",
    "\n",
    "    train_steps_per_epoch = len(train_target_tensor) // batch_size\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_feat_dict,\n",
    "                                                        train_target_tensor)).cache()\n",
    "    train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n",
    "    train_dataset = train_dataset.repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, train_steps_per_epoch\n",
    "\n",
    "train_dict = pd.read_csv(\"./data/temp_data.csv\", error_bad_lines=False)\n",
    "train_feat_dict = {'item_id': train_dict['item_id'],\n",
    "                     'nb_days': train_dict['nb_days']}\n",
    "train_target_tensor = train_dict['target']\n",
    "  \n",
    "train_dataset, train_steps_per_epoch = create_train_tfdata(train_feat_dict,\n",
    "                                                             train_target_tensor,\n",
    "                                                             batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    We redefine our own loss function in order to get rid of the '0' value\n",
    "    which is the one used for padding. This to avoid that the model optimize itself\n",
    "    by predicting this value because it is the padding one.\n",
    "    \n",
    "    :param real: the truth\n",
    "    :param pred: predictions\n",
    "    :return: a masked loss where '0' in real (due to padding)\n",
    "                are not taken into account for the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # to check that pred is numric and not nan\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_object_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                 reduction='none')\n",
    "    loss_ = loss_object_(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def build_model(hp, max_len, item_vocab_size):\n",
    "    \"\"\"\n",
    "    Build a model given the hyper-parameters with item and nb_days input features\n",
    "    :param hp: (kt.HyperParameters) hyper-parameters to use when building this model\n",
    "    :return: built and compiled tensorflow model \n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    inputs['item_id'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
    "                                       name='item_id', dtype=tf.int32)\n",
    "    # create encoding padding mask\n",
    "    encoding_padding_mask = tf.math.logical_not(tf.math.equal(inputs['item_id'], 0))\n",
    "\n",
    "    # nb_days bucketized\n",
    "    inputs['nb_days'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
    "                                       name='nb_days', dtype=tf.int32)\n",
    "\n",
    "    # Pass categorical input through embedding layer\n",
    "    # with size equals to tokenizer vocabulary size\n",
    "    # Remember that vocab_size is len of item tokenizer + 1\n",
    "    # (for the padding '0' value)\n",
    "    \n",
    "    embedding_item = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
    "                                               output_dim=hp.get('embedding_item'),\n",
    "                                               name='embedding_item'\n",
    "                                              )(inputs['item_id'])\n",
    "    # nbins=100, +1 for zero padding\n",
    "    embedding_nb_days = tf.keras.layers.Embedding(input_dim=100 + 1,\n",
    "                                                  output_dim=hp.get('embedding_nb_days'),\n",
    "                                                  name='embedding_nb_days'\n",
    "                                                 )(inputs['nb_days'])\n",
    "\n",
    "    #  Concatenate embedding layers\n",
    "    concat_embedding_input = tf.keras.layers.Concatenate(\n",
    "     name='concat_embedding_input')([embedding_item, embedding_nb_days])\n",
    "\n",
    "    concat_embedding_input = tf.keras.layers.BatchNormalization(\n",
    "     name='batchnorm_inputs')(concat_embedding_input)\n",
    "    \n",
    "    # LSTM layer\n",
    "    rnn = tf.keras.layers.LSTM(units=hp.get('rnn_units_cat'),\n",
    "                                   return_sequences=True,\n",
    "                                   stateful=False,\n",
    "                                   recurrent_initializer='glorot_normal',\n",
    "                                   name='LSTM_cat'\n",
    "                                   )(concat_embedding_input)\n",
    "\n",
    "    rnn = tf.keras.layers.BatchNormalization(name='batchnorm_lstm')(rnn)\n",
    "\n",
    "    # Self attention so key=value in inputs\n",
    "    att = tf.keras.layers.Attention(use_scale=False, causal=True,\n",
    "                                    name='attention')(inputs=[rnn, rnn],\n",
    "                                                      mask=[encoding_padding_mask,\n",
    "                                                            encoding_padding_mask])\n",
    "\n",
    "    # Last layer is a fully connected one\n",
    "    output = tf.keras.layers.Dense(item_vocab_size, name='output')(att)\n",
    "\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss=loss_function,\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, train_dataset, steps_per_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Fit the Keras model on the training dataset for a number of given epochs\n",
    "    :param model: tf model to be trained\n",
    "    :param train_dataset: (tf.data.Dataset object) the training dataset\n",
    "                          used to fit the model\n",
    "    :param steps_per_epoch: (int) Total number of steps (batches of samples) before \n",
    "                            declaring one epoch finished and starting the next epoch.\n",
    "    :param epochs: (int) the number of epochs for the fitting phase\n",
    "    :return: tuple (mirrored_model, history) with trained model and model history\n",
    "    \"\"\"\n",
    "    \n",
    "    # mirrored_strategy allows to use multi GPUs when available\n",
    "    mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "        tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
    "    \n",
    "    with mirrored_strategy.scope():\n",
    "        mirrored_model = model\n",
    "\n",
    "    history = mirrored_model.fit(train_dataset,\n",
    "                                 steps_per_epoch=steps_per_epoch,\n",
    "                                 epochs=epochs, verbose=2)\n",
    "\n",
    "    return mirrored_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\super\\AppData\\Local\\Temp/ipykernel_8396/407096462.py:14: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8396/2585718001.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8396/407096462.py\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(model, train_dataset, steps_per_epoch, epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mmirrored_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     history = mirrored_model.fit(train_dataset,\n\u001b[0m\u001b[0;32m     21\u001b[0m                                  \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                                  epochs=epochs, verbose=2)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "fit_model(build_model, train_dataset, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
